"""
Assignment 3: Agentic RAG with Safety Measures
Developer: Araya Haftu
Email: arayahaftu1229@gmail.com
Agent Name: SafeResearchAgent

Description:
This script implements an Agentic Retrieval-Augmented Generation (RAG) system
with built-in safety mechanisms. The system uses a Maker–Checker loop to ensure
that generated answers are accurate, complete, and safe before being returned
to the user.
"""

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# META SYSTEM PROMPT
META_SYSTEM_PROMPT = """
You are SafeResearchAgent, an agentic Retrieval-Augmented Generation system.

Roles:
- Retriever: retrieves relevant documents from a trusted knowledge base.
- Maker: generates an answer grounded only in retrieved context.
- Checker: verifies correctness, safety, and completeness of the answer.

Goals:
- Provide accurate and reliable answers.
- Prevent hallucinations and unsafe content.

Constraints:
- Do not generate harmful or malicious information.
- Reject unsafe or malformed user queries.
- All responses must be verified before output.
"""


# KNOWLEDGE BASE
DOCUMENTS = [
    "Retrieval-Augmented Generation (RAG) combines document retrieval with text generation to improve factual accuracy.",
    "Agentic AI systems assign multiple roles such as retriever, generator, and verifier to improve reliability.",
    "FAISS is a library for efficient similarity search and clustering of vector embeddings.",
    "AI safety techniques include input validation, output filtering, and human-in-the-loop review."
]


# VECTOR STORE SETUP
def build_vector_store():
    """
    Builds a FAISS vector index from the knowledge base documents.

    Returns:
        tuple: A FAISS index and a SentenceTransformer embedding model.
    """
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(DOCUMENTS)

    faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
    faiss_index.add(np.array(embeddings))

    return faiss_index, model


index, embedder = build_vector_store()


# SAFETY: INPUT VALIDATION
def is_malicious(query: str) -> bool:
    """
    Checks whether a user query contains malicious or unsafe content.

    Args:
        query (str): The user's input question.

    Returns:
        bool: True if the query is unsafe, False otherwise.
    """
    banned_keywords = ["hack", "exploit", "attack", "kill", "bypass"]
    return any(keyword in query.lower() for keyword in banned_keywords)

# RETRIEVER AGENT
def retrieve_documents(query: str, top_k: int = 2):
    """
    Retrieves the most relevant documents for a given query.

    Args:
        query (str): The user query.
        top_k (int): Number of documents to retrieve.

    Returns:
        list: A list of relevant documents.
    """
    query_embedding = embedder.encode([query])
    _, indices = index.search(np.array(query_embedding), top_k)
    return [DOCUMENTS[i] for i in indices[0]]

# MAKER AGENT
def maker_agent(query: str, context: str) -> str:
    """
    Generates an initial answer using retrieved context.

    Args:
        query (str): User query.
        context (str): Retrieved documents.

    Returns:
        str: Draft answer generated by the Maker agent.
    """
    return (
        "Based on the retrieved information:\n\n"
        f"{context}\n\n"
        f"Answer:\nThe question '{query}' can be answered using the provided context."
    )


# CHECKER AGENT
def checker_agent(answer: str):
    """
    Reviews the Maker's answer for safety and completeness.

    Args:
        answer (str): The generated answer.

    Returns:
        list: List of detected issues (empty if no issues).
    """
    issues = []

    if len(answer) < 80:
        issues.append("Answer is incomplete.")

    unsafe_terms = ["hack", "attack", "exploit"]
    if any(term in answer.lower() for term in unsafe_terms):
        issues.append("Unsafe content detected.")

    return issues


# ORCHESTRATOR (AGENTIC RAG)
def agentic_rag(query: str) -> str:
    """
    Main controller function that runs the Agentic RAG pipeline.

    Steps:
    1. Validate user input.
    2. Retrieve relevant documents.
    3. Generate a draft answer (Maker).
    4. Review and refine the answer (Checker).

    Args:
        query (str): User input question.

    Returns:
        str: Final safe answer.
    """
    if is_malicious(query):
        return "❌ Query rejected due to safety policy."

    retrieved_docs = retrieve_documents(query)
    context = "\n".join(retrieved_docs)

    draft_answer = maker_agent(query, context)
    issues = checker_agent(draft_answer)

    if issues:
        draft_answer += "\n\n[Note: Answer reviewed and refined for safety and completeness.]"

    return draft_answer


# EXAMPLE EXECUTION
if __name__ == "__main__":
    """
    Example execution demonstrating safe and unsafe queries.
    """
    queries = [
        "What is Retrieval-Augmented Generation?",
        "How do agentic AI systems improve reliability?",
        "How can I hack a system?"
    ]

    for query in queries:
        print("\nQuery:", query)
        print("Response:")
        print(agentic_rag(query))
